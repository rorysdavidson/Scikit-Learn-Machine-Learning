{"cells":[{"source":"# Machine Learning with Tree-Based Models","metadata":{},"id":"7607bdb2-1707-4361-a984-1c443fe84370","cell_type":"markdown"},{"source":"## Classification with Decision Trees\n\nDecision trees are a type of supervised learning algorithm used for classification tasks. They are called decision trees because they consist of a series of decisions or questions that lead to a final decision or prediction.\n\nIn a decision tree, each internal node represents a feature or attribute, each branch represents a decision rule, and each leaf node represents the outcome or class label. The decision rules are based on the values of the features, and the outcome is determined by following the path from the root node to a leaf node.\n\nThe process of building a decision tree involves selecting the best feature to split the data at each internal node. The goal is to create a tree that can accurately classify new instances by minimizing the impurity or uncertainty of the predictions.\n\nDecision trees have several advantages, including:\n\n- Easy to understand and interpret\n- Can handle both categorical and numerical features\n- Can handle missing values\n- Can handle irrelevant features\n\nHowever, decision trees can also suffer from overfitting, especially when the tree becomes too complex or when the training data is noisy. To mitigate overfitting, techniques such as pruning and setting a maximum depth or minimum number of samples per leaf can be used.\n\nOverall, decision trees are a powerful and versatile algorithm for classification tasks, and they form the basis for more advanced tree-based models such as random forests and gradient boosting.\n","metadata":{},"cell_type":"markdown","id":"61461724-37f7-4a95-884a-cc17307e0d34"},{"source":"import pandas as pd\nimport numpy as np\nwbc = pd.read_csv('datasets/wbc.csv')\nwbc['diagnosis'] = np.where(wbc['diagnosis']=='M',1,0)\n\nX=wbc[['radius_mean', 'concave points_mean']]\ny=wbc['diagnosis']\n\nwbc[['radius_mean', 'concave points_mean', 'diagnosis']].head()","metadata":{"executionCancelledAt":null,"executionTime":3107,"lastExecutedAt":1695714136361,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import pandas as pd\nimport numpy as np\nwbc = pd.read_csv('datasets/wbc.csv')\nwbc['diagnosis'] = np.where(wbc['diagnosis']=='M',1,0)\n\nX=wbc[['radius_mean', 'concave points_mean']]\ny=wbc['diagnosis']\n\nwbc[['radius_mean', 'concave points_mean', 'diagnosis']].head()","outputsMetadata":{"0":{"height":193,"type":"dataFrame"}}},"cell_type":"code","id":"7288d1b5-7217-4815-982a-cf7f5f8f8d8f","execution_count":13,"outputs":[{"output_type":"execute_result","data":{"application/com.datacamp.data-table.v1+json":{"table":{"schema":{"fields":[{"name":"index","type":"integer"},{"name":"radius_mean","type":"number"},{"name":"concave points_mean","type":"number"},{"name":"diagnosis","type":"integer"}],"primaryKey":["index"],"pandas_version":"1.4.0"},"data":[{"index":0,"radius_mean":17.99,"concave points_mean":0.1471,"diagnosis":1},{"index":1,"radius_mean":20.57,"concave points_mean":0.07017,"diagnosis":1},{"index":2,"radius_mean":19.69,"concave points_mean":0.1279,"diagnosis":1},{"index":3,"radius_mean":11.42,"concave points_mean":0.1052,"diagnosis":1},{"index":4,"radius_mean":20.29,"concave points_mean":0.1043,"diagnosis":1}]},"total_rows":5,"truncation_type":null},"text/plain":"   radius_mean  concave points_mean  diagnosis\n0        17.99              0.14710          1\n1        20.57              0.07017          1\n2        19.69              0.12790          1\n3        11.42              0.10520          1\n4        20.29              0.10430          1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>radius_mean</th>\n      <th>concave points_mean</th>\n      <th>diagnosis</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>17.99</td>\n      <td>0.14710</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20.57</td>\n      <td>0.07017</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>19.69</td>\n      <td>0.12790</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>11.42</td>\n      <td>0.10520</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>20.29</td>\n      <td>0.10430</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":13}]},{"source":"\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Split the dataset into 80% train, 20% test\nX_train, X_test, y_train, y_test= train_test_split(X, y,                                                    test_size=0.2,  \n                         stratify=y,                                                                        random_state=1)\n\nSEED = 1\n\n# Instantiate a DecisionTreeClassifier 'dt' with a maximum depth of 6\ndt = DecisionTreeClassifier(max_depth=6, random_state=SEED)\n\n# Fit dt to the training set\ndt.fit(X_train, y_train)\n\n# Predict test set labels\ny_pred = dt.predict(X_test)\nprint(y_pred[0:5])\n","metadata":{"executionCancelledAt":null,"executionTime":15,"lastExecutedAt":1695714000710,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Split the dataset into 80% train, 20% test\nX_train, X_test, y_train, y_test= train_test_split(X, y,                                                    test_size=0.2,  \n                         stratify=y,                                                                        random_state=1)\n\nSEED = 1\n\n# Instantiate a DecisionTreeClassifier 'dt' with a maximum depth of 6\ndt = DecisionTreeClassifier(max_depth=6, random_state=SEED)\n\n# Fit dt to the training set\ndt.fit(X_train, y_train)\n\n# Predict test set labels\ny_pred = dt.predict(X_test)\nprint(y_pred[0:5])\n","outputsMetadata":{"0":{"height":37,"type":"stream"}}},"cell_type":"code","id":"f79d1205-1647-4b4e-a427-c2eefaf45b21","execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":"[0 0 0 1 0]\n"}]},{"source":"# Import accuracy_score\nfrom sklearn.metrics import accuracy_score\n\n# Predict test set labels\ny_pred = dt.predict(X_test)\n\n# Compute test set accuracy  \nacc = accuracy_score(y_pred, y_test)\nprint(\"Test set accuracy: {:.2f}\".format(acc))","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":37,"type":"stream"}}},"cell_type":"code","id":"d100580b-d7bc-4adb-acd3-8bf7fd99ebf0","execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":"Test set accuracy: 0.89\n"}]},{"source":"### Logistic regression vs classification tree\nA classification tree divides the feature space into rectangular regions. In contrast, a linear model such as logistic regression produces only a single linear decision boundary dividing the feature space into two decision regions.\n\n```\n# Import LogisticRegression from sklearn.linear_model\nfrom sklearn.linear_model import  LogisticRegression\n\n# Instatiate logreg\nlogreg = LogisticRegression(random_state=1)\n\n# Fit logreg to the training set\nlogreg.fit(X_train, y_train)\n\n# Define a list called clfs containing the two classifiers logreg and dt\nclfs = [logreg, dt]\n\n# Review the decision regions of the two classifiers\nplot_labeled_decision_regions(X_test, y_test, clfs)\n```\n\n![Uploading image.png]()\n","metadata":{},"cell_type":"markdown","id":"75c2dc34-649f-4977-8510-0c7c9fa167b2"},{"source":"## Decision Tree Learning\n\nThe decision tree is a flowchart-like structure where each internal node represents a feature or attribute, each branch represents a decision rule, and each leaf node represents the outcome or class label.\n\nThe process of building a decision tree involves recursively partitioning the training data based on the values of the features. The goal is to create partitions that are as pure as possible, meaning that they contain mostly samples of a single class.\n\nTo determine the best feature to split on at each internal node, various criteria can be used, such as Gini impurity or information gain. The splitting process continues until a stopping criterion is met, such as reaching a maximum depth or having a minimum number of samples in a node.\n\nOnce the decision tree is built, it can be used to make predictions on new, unseen data by traversing the tree from the root node to a leaf node based on the values of the features.\n","metadata":{},"cell_type":"markdown","id":"d3ccf6c4-a4fd-42bd-84a6-79555eb7a9a6"},{"source":"# Import DecisionTreeClassifier from sklearn.tree\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Instantiate dt_entropy, set 'entropy' as the information criterion\ndt_entropy = DecisionTreeClassifier(max_depth=8, criterion='entropy', random_state=1)\n\n# Fit dt_entropy to the training set\ndt_entropy.fit(X_train, y_train)\n\n# Instantiate dt_entropy, set 'entropy' as the information criterion\ndt_gini = DecisionTreeClassifier(max_depth=8, criterion='gini', random_state=1)\n\n# Fit dt_entropy to the training set\ndt_gini.fit(X_train, y_train)","metadata":{"executionCancelledAt":null,"executionTime":16,"lastExecutedAt":1695715504182,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import DecisionTreeClassifier from sklearn.tree\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Instantiate dt_entropy, set 'entropy' as the information criterion\ndt_entropy = DecisionTreeClassifier(max_depth=8, criterion='entropy', random_state=1)\n\n# Fit dt_entropy to the training set\ndt_entropy.fit(X_train, y_train)\n\n# Instantiate dt_entropy, set 'entropy' as the information criterion\ndt_gini = DecisionTreeClassifier(max_depth=8, criterion='gini', random_state=1)\n\n# Fit dt_entropy to the training set\ndt_gini.fit(X_train, y_train)"},"cell_type":"code","id":"94f7662a-6e3d-43a8-a4b0-9b1c3d739d09","execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":"DecisionTreeClassifier(max_depth=8, random_state=1)","text/html":"<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(max_depth=8, random_state=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(max_depth=8, random_state=1)</pre></div></div></div></div></div>"},"metadata":{},"execution_count":19}]},{"source":"# Import accuracy_score from sklearn.metrics\nfrom sklearn.metrics import accuracy_score\n\n# Use dt_entropy to predict test set labels\ny_pred = dt_entropy.predict(X_test)\n\n# Evaluate accuracy_entropy\naccuracy_entropy = accuracy_score(y_pred, y_test)\n\n# Use dt_entropy to predict test set labels\ny_pred = dt_gini.predict(X_test)\n\n# Evaluate accuracy_entropy\naccuracy_gini = accuracy_score(y_pred, y_test)\n\n# Print accuracy_entropy\nprint(f'Accuracy achieved by using entropy: {accuracy_entropy:.3f}')\n\n# Print accuracy_gini\nprint(f'Accuracy achieved by using the gini index: {accuracy_gini:.3f}')","metadata":{"executionCancelledAt":null,"executionTime":13,"lastExecutedAt":1695715511883,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import accuracy_score from sklearn.metrics\nfrom sklearn.metrics import accuracy_score\n\n# Use dt_entropy to predict test set labels\ny_pred = dt_entropy.predict(X_test)\n\n# Evaluate accuracy_entropy\naccuracy_entropy = accuracy_score(y_pred, y_test)\n\n# Use dt_entropy to predict test set labels\ny_pred = dt_gini.predict(X_test)\n\n# Evaluate accuracy_entropy\naccuracy_gini = accuracy_score(y_pred, y_test)\n\n# Print accuracy_entropy\nprint(f'Accuracy achieved by using entropy: {accuracy_entropy:.3f}')\n\n# Print accuracy_gini\nprint(f'Accuracy achieved by using the gini index: {accuracy_gini:.3f}')","outputsMetadata":{"0":{"height":56,"type":"stream"}}},"cell_type":"code","id":"1312fed1-0b3d-4c7b-9b70-f6cf30ad6bbf","execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":"Accuracy achieved by using entropy: 0.886\nAccuracy achieved by using the gini index: 0.921\n"}]},{"source":"## Decision Trees for Regression\n\nDecision trees can also be used for regression tasks. Instead of predicting classes, decision trees for regression predict continuous values.\n","metadata":{},"cell_type":"markdown","id":"9ca49aeb-fcf3-4f2a-a93a-18e7fcd4252e"},{"source":"import pandas as pd\nimport numpy as np\ndf = pd.read_csv('datasets/auto.csv')\n\nX=df.drop('mpg', axis=1)\ny=df['mpg']\n\nX.head()","metadata":{"executionCancelledAt":null,"executionTime":36,"lastExecutedAt":1695722147579,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import pandas as pd\nimport numpy as np\ndf = pd.read_csv('datasets/auto.csv')\n\nX=df.drop('mpg', axis=1)\ny=df['mpg']\n\nX.head()","outputsMetadata":{"0":{"height":183,"type":"dataFrame"}}},"cell_type":"code","id":"74493c8b-234a-4e2f-b006-b1355fe87bb8","execution_count":3,"outputs":[{"output_type":"execute_result","data":{"application/com.datacamp.data-table.v1+json":{"table":{"schema":{"fields":[{"name":"index","type":"integer"},{"name":"displ","type":"number"},{"name":"hp","type":"integer"},{"name":"weight","type":"integer"},{"name":"accel","type":"number"},{"name":"origin","type":"string"},{"name":"size","type":"number"}],"primaryKey":["index"],"pandas_version":"1.4.0"},"data":[{"index":0,"displ":250,"hp":88,"weight":3139,"accel":14.5,"origin":"US","size":15},{"index":1,"displ":304,"hp":193,"weight":4732,"accel":18.5,"origin":"US","size":20},{"index":2,"displ":91,"hp":60,"weight":1800,"accel":16.4,"origin":"Asia","size":10},{"index":3,"displ":250,"hp":98,"weight":3525,"accel":19,"origin":"US","size":15},{"index":4,"displ":97,"hp":78,"weight":2188,"accel":15.8,"origin":"Europe","size":10}]},"total_rows":5,"truncation_type":null},"text/plain":"   displ   hp  weight  accel  origin  size\n0  250.0   88    3139   14.5      US  15.0\n1  304.0  193    4732   18.5      US  20.0\n2   91.0   60    1800   16.4    Asia  10.0\n3  250.0   98    3525   19.0      US  15.0\n4   97.0   78    2188   15.8  Europe  10.0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>displ</th>\n      <th>hp</th>\n      <th>weight</th>\n      <th>accel</th>\n      <th>origin</th>\n      <th>size</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>250.0</td>\n      <td>88</td>\n      <td>3139</td>\n      <td>14.5</td>\n      <td>US</td>\n      <td>15.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>304.0</td>\n      <td>193</td>\n      <td>4732</td>\n      <td>18.5</td>\n      <td>US</td>\n      <td>20.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>91.0</td>\n      <td>60</td>\n      <td>1800</td>\n      <td>16.4</td>\n      <td>Asia</td>\n      <td>10.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>250.0</td>\n      <td>98</td>\n      <td>3525</td>\n      <td>19.0</td>\n      <td>US</td>\n      <td>15.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>97.0</td>\n      <td>78</td>\n      <td>2188</td>\n      <td>15.8</td>\n      <td>Europe</td>\n      <td>10.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":3}]},{"source":"# Importing LabelEncoder from Sklearn\n# library from preprocessing Module.\nfrom sklearn.preprocessing import LabelEncoder\n \n# Creating a instance of label Encoder.\nle = LabelEncoder()\n \n# Using .fit_transform function to fit label\n# encoder and return encoded label\nlabel = le.fit_transform(X['origin'])\n \n# removing the column 'Purchased' from df\n# as it is of no use now.\nX.drop(\"origin\", axis=1, inplace=True)\n \n# Appending the array to our dataFrame\n# with column name 'Purchased'\nX[\"origin\"] = label\n \n# printing Dataframe\nX","metadata":{"executionCancelledAt":null,"executionTime":71,"lastExecutedAt":1695722151968,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Importing LabelEncoder from Sklearn\n# library from preprocessing Module.\nfrom sklearn.preprocessing import LabelEncoder\n \n# Creating a instance of label Encoder.\nle = LabelEncoder()\n \n# Using .fit_transform function to fit label\n# encoder and return encoded label\nlabel = le.fit_transform(X['origin'])\n \n# removing the column 'Purchased' from df\n# as it is of no use now.\nX.drop(\"origin\", axis=1, inplace=True)\n \n# Appending the array to our dataFrame\n# with column name 'Purchased'\nX[\"origin\"] = label\n \n# printing Dataframe\nX","outputsMetadata":{"0":{"height":305,"type":"dataFrame"}}},"cell_type":"code","id":"1d8daf21-6194-48d6-8583-577ca5d11722","execution_count":4,"outputs":[{"output_type":"execute_result","data":{"application/com.datacamp.data-table.v1+json":{"table":{"schema":{"fields":[{"name":"index","type":"integer"},{"name":"displ","type":"number"},{"name":"hp","type":"integer"},{"name":"weight","type":"integer"},{"name":"accel","type":"number"},{"name":"size","type":"number"},{"name":"origin","type":"integer"}],"primaryKey":["index"],"pandas_version":"1.4.0"},"data":[{"index":0,"displ":250,"hp":88,"weight":3139,"accel":14.5,"size":15,"origin":2},{"index":1,"displ":304,"hp":193,"weight":4732,"accel":18.5,"size":20,"origin":2},{"index":2,"displ":91,"hp":60,"weight":1800,"accel":16.4,"size":10,"origin":0},{"index":3,"displ":250,"hp":98,"weight":3525,"accel":19,"size":15,"origin":2},{"index":4,"displ":97,"hp":78,"weight":2188,"accel":15.8,"size":10,"origin":1},{"index":5,"displ":119,"hp":100,"weight":2615,"accel":14.8,"size":10,"origin":0},{"index":6,"displ":108,"hp":75,"weight":2265,"accel":15.2,"size":10,"origin":0},{"index":7,"displ":121,"hp":76,"weight":2511,"accel":18,"size":10,"origin":1},{"index":8,"displ":302,"hp":130,"weight":4295,"accel":14.9,"size":20,"origin":2},{"index":9,"displ":302,"hp":140,"weight":3449,"accel":10.5,"size":20,"origin":2},{"index":10,"displ":97,"hp":52,"weight":2130,"accel":24.6,"size":10,"origin":1},{"index":11,"displ":151,"hp":88,"weight":2740,"accel":16,"size":10,"origin":2},{"index":12,"displ":135,"hp":84,"weight":2295,"accel":11.6,"size":10,"origin":2},{"index":13,"displ":351,"hp":148,"weight":4657,"accel":13.5,"size":20,"origin":2},{"index":14,"displ":400,"hp":150,"weight":3761,"accel":9.5,"size":20,"origin":2},{"index":15,"displ":307,"hp":130,"weight":4098,"accel":14,"size":20,"origin":2},{"index":16,"displ":79,"hp":58,"weight":1825,"accel":18.6,"size":10,"origin":1},{"index":17,"displ":119,"hp":82,"weight":2720,"accel":19.4,"size":10,"origin":2},{"index":18,"displ":71,"hp":65,"weight":1836,"accel":21,"size":10,"origin":0},{"index":19,"displ":80,"hp":110,"weight":2720,"accel":13.5,"size":7.5,"origin":0},{"index":20,"displ":225,"hp":95,"weight":3264,"accel":16,"size":15,"origin":2},{"index":21,"displ":231,"hp":110,"weight":3907,"accel":21,"size":15,"origin":2},{"index":22,"displ":302,"hp":140,"weight":4141,"accel":14,"size":20,"origin":2},{"index":23,"displ":383,"hp":170,"weight":3563,"accel":10,"size":20,"origin":2},{"index":24,"displ":140,"hp":78,"weight":2592,"accel":18.5,"size":10,"origin":2},{"index":25,"displ":98,"hp":90,"weight":2265,"accel":15.5,"size":10,"origin":1},{"index":26,"displ":144,"hp":96,"weight":2665,"accel":13.9,"size":10,"origin":0},{"index":27,"displ":113,"hp":95,"weight":2372,"accel":15,"size":10,"origin":0},{"index":28,"displ":231,"hp":110,"weight":3039,"accel":15,"size":15,"origin":2},{"index":29,"displ":120,"hp":75,"weight":2542,"accel":17.5,"size":10,"origin":0},{"index":30,"displ":168,"hp":132,"weight":2910,"accel":11.4,"size":15,"origin":0},{"index":31,"displ":304,"hp":150,"weight":3892,"accel":12.5,"size":20,"origin":2},{"index":32,"displ":140,"hp":83,"weight":2639,"accel":17,"size":10,"origin":2},{"index":33,"displ":225,"hp":85,"weight":3465,"accel":16.6,"size":15,"origin":2},{"index":34,"displ":107,"hp":86,"weight":2464,"accel":15.5,"size":10,"origin":1},{"index":35,"displ":116,"hp":75,"weight":2158,"accel":15.5,"size":10,"origin":1},{"index":36,"displ":302,"hp":140,"weight":4638,"accel":16,"size":20,"origin":2},{"index":37,"displ":302,"hp":139,"weight":3205,"accel":11.2,"size":20,"origin":2},{"index":38,"displ":98,"hp":70,"weight":2125,"accel":17.3,"size":10,"origin":2},{"index":39,"displ":85,"hp":52,"weight":2035,"accel":22.2,"size":10,"origin":2},{"index":40,"displ":81,"hp":60,"weight":1760,"accel":16.1,"size":10,"origin":0},{"index":41,"displ":135,"hp":84,"weight":2370,"accel":13,"size":10,"origin":2},{"index":42,"displ":351,"hp":138,"weight":3955,"accel":13.2,"size":20,"origin":2},{"index":43,"displ":400,"hp":180,"weight":4220,"accel":11.1,"size":20,"origin":2},{"index":44,"displ":98,"hp":65,"weight":2380,"accel":20.7,"size":10,"origin":2},{"index":45,"displ":79,"hp":67,"weight":1950,"accel":19,"size":10,"origin":0},{"index":46,"displ":119,"hp":97,"weight":2300,"accel":14.7,"size":10,"origin":0},{"index":47,"displ":318,"hp":150,"weight":4457,"accel":13.5,"size":20,"origin":2},{"index":48,"displ":98,"hp":70,"weight":2120,"accel":15.5,"size":10,"origin":2},{"index":49,"displ":250,"hp":100,"weight":3336,"accel":17,"size":15,"origin":2},{"index":50,"displ":383,"hp":180,"weight":4955,"accel":11.5,"size":20,"origin":2},{"index":51,"displ":302,"hp":129,"weight":3725,"accel":13.4,"size":20,"origin":2},{"index":52,"displ":113,"hp":95,"weight":2228,"accel":14,"size":10,"origin":0},{"index":53,"displ":151,"hp":90,"weight":2670,"accel":16,"size":10,"origin":2},{"index":54,"displ":98,"hp":83,"weight":2219,"accel":16.5,"size":10,"origin":1},{"index":55,"displ":105,"hp":75,"weight":2230,"accel":14.5,"size":10,"origin":2},{"index":56,"displ":232,"hp":100,"weight":2914,"accel":16,"size":15,"origin":2},{"index":57,"displ":200,"hp":85,"weight":3070,"accel":16.7,"size":15,"origin":2},{"index":58,"displ":232,"hp":112,"weight":2835,"accel":14.7,"size":15,"origin":2},{"index":59,"displ":91,"hp":67,"weight":1965,"accel":15,"size":10,"origin":0},{"index":60,"displ":71,"hp":65,"weight":1773,"accel":19,"size":10,"origin":0},{"index":61,"displ":250,"hp":88,"weight":3302,"accel":15.5,"size":15,"origin":2},{"index":62,"displ":250,"hp":100,"weight":3781,"accel":17,"size":15,"origin":2},{"index":63,"displ":140,"hp":75,"weight":2542,"accel":17,"size":10,"origin":2},{"index":64,"displ":225,"hp":100,"weight":3233,"accel":15.4,"size":15,"origin":2},{"index":65,"displ":91,"hp":70,"weight":1955,"accel":20.5,"size":10,"origin":2},{"index":66,"displ":350,"hp":145,"weight":4055,"accel":12,"size":20,"origin":2},{"index":67,"displ":260,"hp":110,"weight":3365,"accel":15.5,"size":20,"origin":2},{"index":68,"displ":318,"hp":210,"weight":4382,"accel":13.5,"size":20,"origin":2},{"index":69,"displ":98,"hp":80,"weight":2164,"accel":15,"size":10,"origin":2},{"index":70,"displ":318,"hp":145,"weight":4140,"accel":13.7,"size":20,"origin":2},{"index":71,"displ":96,"hp":69,"weight":2189,"accel":18,"size":10,"origin":1},{"index":72,"displ":318,"hp":150,"weight":4237,"accel":14.5,"size":20,"origin":2},{"index":73,"displ":429,"hp":198,"weight":4952,"accel":11.5,"size":20,"origin":2},{"index":74,"displ":146,"hp":120,"weight":2930,"accel":13.8,"size":15,"origin":0},{"index":75,"displ":140,"hp":92,"weight":2572,"accel":14.9,"size":10,"origin":2},{"index":76,"displ":232,"hp":90,"weight":3085,"accel":17.6,"size":15,"origin":2},{"index":77,"displ":173,"hp":115,"weight":2700,"accel":12.9,"size":15,"origin":2},{"index":78,"displ":198,"hp":95,"weight":2904,"accel":16,"size":15,"origin":2},{"index":79,"displ":116,"hp":75,"weight":2246,"accel":14,"size":10,"origin":1},{"index":80,"displ":145,"hp":76,"weight":3160,"accel":19.6,"size":15,"origin":1},{"index":81,"displ":79,"hp":67,"weight":2000,"accel":16,"size":10,"origin":1},{"index":82,"displ":141,"hp":71,"weight":3190,"accel":24.8,"size":10,"origin":1},{"index":83,"displ":231,"hp":115,"weight":3245,"accel":15.4,"size":15,"origin":2},{"index":84,"displ":135,"hp":84,"weight":2525,"accel":16,"size":10,"origin":2},{"index":85,"displ":114,"hp":91,"weight":2582,"accel":14,"size":10,"origin":1},{"index":86,"displ":350,"hp":150,"weight":4699,"accel":14.5,"size":20,"origin":2},{"index":87,"displ":440,"hp":215,"weight":4312,"accel":8.5,"size":20,"origin":2},{"index":88,"displ":91,"hp":67,"weight":1995,"accel":16.2,"size":10,"origin":0},{"index":89,"displ":400,"hp":175,"weight":5140,"accel":12,"size":20,"origin":2},{"index":90,"displ":98,"hp":60,"weight":2164,"accel":22.1,"size":10,"origin":2},{"index":91,"displ":350,"hp":175,"weight":4100,"accel":13,"size":20,"origin":2},{"index":92,"displ":181,"hp":110,"weight":2945,"accel":16.4,"size":15,"origin":2},{"index":93,"displ":113,"hp":95,"weight":2278,"accel":15.5,"size":10,"origin":0},{"index":94,"displ":91,"hp":68,"weight":1985,"accel":16,"size":10,"origin":0},{"index":95,"displ":400,"hp":150,"weight":4464,"accel":12,"size":20,"origin":2},{"index":96,"displ":91,"hp":67,"weight":1850,"accel":13.8,"size":10,"origin":0},{"index":97,"displ":200,"hp":95,"weight":3155,"accel":18.2,"size":15,"origin":2},{"index":98,"displ":258,"hp":110,"weight":2962,"accel":13.5,"size":15,"origin":2},{"index":99,"displ":156,"hp":105,"weight":2745,"accel":16.7,"size":10,"origin":2},{"index":100,"displ":130,"hp":102,"weight":3150,"accel":15.7,"size":10,"origin":1},{"index":101,"displ":121,"hp":110,"weight":2660,"accel":14,"size":10,"origin":1},{"index":102,"displ":140,"hp":89,"weight":2755,"accel":15.8,"size":10,"origin":2},{"index":103,"displ":98,"hp":66,"weight":1800,"accel":14.4,"size":10,"origin":2},{"index":104,"displ":120,"hp":88,"weight":2957,"accel":17,"size":10,"origin":1},{"index":105,"displ":90,"hp":75,"weight":2108,"accel":15.5,"size":10,"origin":1},{"index":106,"displ":250,"hp":78,"weight":3574,"accel":21,"size":15,"origin":2},{"index":107,"displ":350,"hp":105,"weight":3725,"accel":19,"size":20,"origin":2},{"index":108,"displ":85,"hp":70,"weight":1990,"accel":17,"size":10,"origin":0},{"index":109,"displ":131,"hp":103,"weight":2830,"accel":15.9,"size":12.5,"origin":1},{"index":110,"displ":97,"hp":60,"weight":1834,"accel":19,"size":10,"origin":1},{"index":111,"displ":304,"hp":150,"weight":3672,"accel":11.5,"size":20,"origin":2},{"index":112,"displ":140,"hp":72,"weight":2401,"accel":19.5,"size":10,"origin":2},{"index":113,"displ":400,"hp":170,"weight":4746,"accel":12,"size":20,"origin":2},{"index":114,"displ":107,"hp":90,"weight":2430,"accel":14.5,"size":10,"origin":1},{"index":115,"displ":260,"hp":110,"weight":4060,"accel":19,"size":20,"origin":2},{"index":116,"displ":79,"hp":58,"weight":1755,"accel":16.9,"size":10,"origin":0},{"index":117,"displ":351,"hp":152,"weight":4215,"accel":12.8,"size":20,"origin":2},{"index":118,"displ":350,"hp":145,"weight":3988,"accel":13,"size":20,"origin":2},{"index":119,"displ":302,"hp":139,"weight":3570,"accel":12.8,"size":20,"origin":2},{"index":120,"displ":101,"hp":83,"weight":2202,"accel":15.3,"size":10,"origin":1},{"index":121,"displ":72,"hp":69,"weight":1613,"accel":18,"size":10,"origin":0},{"index":122,"displ":318,"hp":150,"weight":3777,"accel":12.5,"size":20,"origin":2},{"index":123,"displ":121,"hp":67,"weight":2950,"accel":19.9,"size":12.5,"origin":1},{"index":124,"displ":111,"hp":80,"weight":2155,"accel":14.8,"size":10,"origin":2},{"index":125,"displ":89,"hp":71,"weight":1925,"accel":14,"size":10,"origin":1},{"index":126,"displ":97,"hp":46,"weight":1835,"accel":20.5,"size":10,"origin":1},{"index":127,"displ":225,"hp":105,"weight":3439,"accel":15.5,"size":15,"origin":2},{"index":128,"displ":140,"hp":90,"weight":2408,"accel":19.5,"size":10,"origin":2},{"index":129,"displ":225,"hp":110,"weight":3620,"accel":18.7,"size":15,"origin":2},{"index":130,"displ":400,"hp":175,"weight":4464,"accel":11.5,"size":20,"origin":2},{"index":131,"displ":97.5,"hp":80,"weight":2126,"accel":17,"size":10,"origin":2},{"index":132,"displ":105,"hp":74,"weight":2190,"accel":14.2,"size":10,"origin":1},{"index":133,"displ":318,"hp":150,"weight":4096,"accel":13,"size":20,"origin":2},{"index":134,"displ":360,"hp":150,"weight":3940,"accel":13,"size":20,"origin":2},{"index":135,"displ":86,"hp":65,"weight":2019,"accel":16.4,"size":10,"origin":0},{"index":136,"displ":232,"hp":100,"weight":3288,"accel":15.5,"size":15,"origin":2},{"index":137,"displ":90,"hp":48,"weight":2085,"accel":21.7,"size":10,"origin":1},{"index":138,"displ":250,"hp":105,"weight":3459,"accel":16,"size":15,"origin":2},{"index":139,"displ":140,"hp":90,"weight":2264,"accel":15.5,"size":10,"origin":2},{"index":140,"displ":90,"hp":48,"weight":2335,"accel":23.7,"size":10,"origin":1},{"index":141,"displ":231,"hp":105,"weight":3380,"accel":15.8,"size":15,"origin":2},{"index":142,"displ":231,"hp":105,"weight":3535,"accel":19.2,"size":15,"origin":2},{"index":143,"displ":140,"hp":88,"weight":2870,"accel":18.1,"size":10,"origin":2},{"index":144,"displ":232,"hp":100,"weight":2945,"accel":16,"size":15,"origin":2},{"index":145,"displ":97,"hp":75,"weight":2155,"accel":16.4,"size":10,"origin":0},{"index":146,"displ":121,"hp":113,"weight":2234,"accel":12.5,"size":10,"origin":1},{"index":147,"displ":400,"hp":190,"weight":4422,"accel":12.5,"size":20,"origin":2},{"index":148,"displ":156,"hp":92,"weight":2620,"accel":14.4,"size":10,"origin":2},{"index":149,"displ":141,"hp":80,"weight":3230,"accel":20.4,"size":10,"origin":1},{"index":150,"displ":350,"hp":165,"weight":4274,"accel":12,"size":20,"origin":2},{"index":151,"displ":350,"hp":180,"weight":4380,"accel":12.1,"size":20,"origin":2},{"index":152,"displ":89,"hp":71,"weight":1990,"accel":14.9,"size":10,"origin":1},{"index":153,"displ":120,"hp":97,"weight":2489,"accel":15,"size":10,"origin":0},{"index":154,"displ":250,"hp":72,"weight":3158,"accel":19.5,"size":15,"origin":2},{"index":155,"displ":225,"hp":105,"weight":3613,"accel":16.5,"size":15,"origin":2},{"index":156,"displ":151,"hp":90,"weight":2556,"accel":13.2,"size":10,"origin":2},{"index":157,"displ":108,"hp":75,"weight":2350,"accel":16.8,"size":10,"origin":0},{"index":158,"displ":97,"hp":88,"weight":2130,"accel":14.5,"size":10,"origin":0},{"index":159,"displ":350,"hp":155,"weight":4502,"accel":13.5,"size":20,"origin":2},{"index":160,"displ":91,"hp":68,"weight":1970,"accel":17.6,"size":10,"origin":0},{"index":161,"displ":151,"hp":90,"weight":2678,"accel":16.5,"size":10,"origin":2},{"index":162,"displ":135,"hp":84,"weight":2490,"accel":15.7,"size":10,"origin":2},{"index":163,"displ":120,"hp":87,"weight":2979,"accel":19.5,"size":10,"origin":1},{"index":164,"displ":121,"hp":112,"weight":2868,"accel":15.5,"size":10,"origin":1},{"index":165,"displ":110,"hp":87,"weight":2672,"accel":17.5,"size":10,"origin":1},{"index":166,"displ":350,"hp":125,"weight":3900,"accel":17.4,"size":20,"origin":2},{"index":167,"displ":156,"hp":108,"weight":2930,"accel":15.5,"size":15,"origin":0},{"index":168,"displ":351,"hp":142,"weight":4054,"accel":14.3,"size":20,"origin":2},{"index":169,"displ":119,"hp":97,"weight":2405,"accel":14.9,"size":10,"origin":0},{"index":170,"displ":250,"hp":105,"weight":3353,"accel":14.5,"size":15,"origin":2},{"index":171,"displ":97,"hp":75,"weight":2171,"accel":16,"size":10,"origin":0},{"index":172,"displ":302,"hp":137,"weight":4042,"accel":14.5,"size":20,"origin":2},{"index":173,"displ":318,"hp":150,"weight":3399,"accel":11,"size":20,"origin":2},{"index":174,"displ":112,"hp":88,"weight":2640,"accel":18.6,"size":10,"origin":2},{"index":175,"displ":350,"hp":145,"weight":4440,"accel":14,"size":20,"origin":2},{"index":176,"displ":98,"hp":63,"weight":2051,"accel":17,"size":10,"origin":2},{"index":177,"displ":104,"hp":95,"weight":2375,"accel":17.5,"size":10,"origin":1},{"index":178,"displ":305,"hp":140,"weight":4215,"accel":13,"size":20,"origin":2},{"index":179,"displ":112,"hp":88,"weight":2395,"accel":18,"size":10,"origin":2},{"index":180,"displ":262,"hp":85,"weight":3015,"accel":17,"size":15,"origin":2},{"index":181,"displ":79,"hp":70,"weight":2074,"accel":19.5,"size":10,"origin":1},{"index":182,"displ":200,"hp":85,"weight":2990,"accel":18.2,"size":15,"origin":2},{"index":183,"displ":121,"hp":115,"weight":2671,"accel":13.5,"size":10,"origin":1},{"index":184,"displ":122,"hp":86,"weight":2226,"accel":16.5,"size":10,"origin":2},{"index":185,"displ":98,"hp":79,"weight":2255,"accel":17.7,"size":10,"origin":2},{"index":186,"displ":168,"hp":120,"weight":3820,"accel":16.7,"size":15,"origin":1},{"index":187,"displ":258,"hp":120,"weight":3410,"accel":15.1,"size":15,"origin":2},{"index":188,"displ":86,"hp":65,"weight":2110,"accel":17.9,"size":10,"origin":0},{"index":189,"displ":121,"hp":110,"weight":2600,"accel":12.8,"size":10,"origin":1},{"index":190,"displ":454,"hp":220,"weight":4354,"accel":9,"size":20,"origin":2},{"index":191,"displ":121,"hp":115,"weight":2795,"accel":15.7,"size":10,"origin":1},{"index":192,"displ":350,"hp":170,"weight":4165,"accel":11.4,"size":20,"origin":2},{"index":193,"displ":225,"hp":100,"weight":3430,"accel":17.2,"size":15,"origin":2},{"index":194,"displ":260,"hp":90,"weight":3420,"accel":22.2,"size":20,"origin":2},{"index":195,"displ":455,"hp":225,"weight":4951,"accel":11,"size":20,"origin":2},{"index":196,"displ":200,"hp":85,"weight":2965,"accel":15.8,"size":15,"origin":2},{"index":197,"displ":98,"hp":65,"weight":2045,"accel":16.2,"size":10,"origin":2},{"index":198,"displ":120,"hp":97,"weight":2506,"accel":14.5,"size":10,"origin":0},{"index":199,"displ":151,"hp":90,"weight":3003,"accel":20.1,"size":10,"origin":2},{"index":200,"displ":232,"hp":90,"weight":3211,"accel":17,"size":15,"origin":2},{"index":201,"displ":68,"hp":49,"weight":1867,"accel":19.5,"size":10,"origin":1},{"index":202,"displ":173,"hp":110,"weight":2725,"accel":12.6,"size":15,"origin":2},{"index":203,"displ":108,"hp":70,"weight":2245,"accel":16.9,"size":10,"origin":0},{"index":204,"displ":119,"hp":92,"weight":2434,"accel":15,"size":10,"origin":0},{"index":205,"displ":91,"hp":53,"weight":1795,"accel":17.5,"size":10,"origin":0},{"index":206,"displ":232,"hp":100,"weight":2789,"accel":15,"size":15,"origin":2},{"index":207,"displ":390,"hp":190,"weight":3850,"accel":8.5,"size":20,"origin":2},{"index":208,"displ":105,"hp":63,"weight":2215,"accel":14.9,"size":10,"origin":2},{"index":209,"displ":232,"hp":90,"weight":3210,"accel":17.2,"size":15,"origin":2},{"index":210,"displ":91,"hp":67,"weight":1965,"accel":15.7,"size":10,"origin":0},{"index":211,"displ":86,"hp":65,"weight":1975,"accel":15.2,"size":10,"origin":0},{"index":212,"displ":107,"hp":75,"weight":2210,"accel":14.4,"size":10,"origin":0},{"index":213,"displ":225,"hp":100,"weight":3651,"accel":17.7,"size":15,"origin":2},{"index":214,"displ":258,"hp":110,"weight":3730,"accel":19,"size":15,"origin":2},{"index":215,"displ":89,"hp":60,"weight":1968,"accel":18.8,"size":10,"origin":0},{"index":216,"displ":108,"hp":93,"weight":2391,"accel":15.5,"size":10,"origin":0},{"index":217,"displ":97,"hp":88,"weight":2100,"accel":16.5,"size":10,"origin":0},{"index":218,"displ":318,"hp":150,"weight":4498,"accel":14.5,"size":20,"origin":2},{"index":219,"displ":250,"hp":100,"weight":3329,"accel":15.5,"size":15,"origin":2},{"index":220,"displ":318,"hp":150,"weight":3940,"accel":13.2,"size":20,"origin":2},{"index":221,"displ":112,"hp":88,"weight":2605,"accel":19.6,"size":10,"origin":2},{"index":222,"displ":455,"hp":225,"weight":3086,"accel":10,"size":20,"origin":2},{"index":223,"displ":98,"hp":68,"weight":2045,"accel":18.5,"size":10,"origin":0},{"index":224,"displ":105,"hp":70,"weight":2150,"accel":14.9,"size":10,"origin":2},{"index":225,"displ":429,"hp":208,"weight":4633,"accel":11,"size":20,"origin":2},{"index":226,"displ":250,"hp":105,"weight":3897,"accel":18.5,"size":15,"origin":2},{"index":227,"displ":120,"hp":74,"weight":2635,"accel":18.3,"size":10,"origin":0},{"index":228,"displ":225,"hp":90,"weight":3381,"accel":18.7,"size":15,"origin":2},{"index":229,"displ":250,"hp":110,"weight":3645,"accel":16.2,"size":15,"origin":2},{"index":230,"displ":250,"hp":72,"weight":3432,"accel":21,"size":15,"origin":2},{"index":231,"displ":199,"hp":97,"weight":2774,"accel":15.5,"size":15,"origin":2},{"index":232,"displ":122,"hp":88,"weight":2500,"accel":15.1,"size":10,"origin":1},{"index":233,"displ":200,"hp":88,"weight":3060,"accel":17.1,"size":15,"origin":2},{"index":234,"displ":302,"hp":129,"weight":3169,"accel":12,"size":20,"origin":2},{"index":235,"displ":112,"hp":85,"weight":2575,"accel":16.2,"size":10,"origin":2},{"index":236,"displ":122,"hp":86,"weight":2395,"accel":16,"size":10,"origin":2},{"index":237,"displ":400,"hp":150,"weight":4997,"accel":14,"size":20,"origin":2},{"index":238,"displ":85,"hp":70,"weight":1945,"accel":16.8,"size":10,"origin":0},{"index":239,"displ":90,"hp":48,"weight":1985,"accel":21.5,"size":10,"origin":1},{"index":240,"displ":183,"hp":77,"weight":3530,"accel":20.1,"size":12.5,"origin":1},{"index":241,"displ":85,"hp":65,"weight":2110,"accel":19.2,"size":10,"origin":0},{"index":242,"displ":400,"hp":175,"weight":4385,"accel":12,"size":20,"origin":2},{"index":243,"displ":134,"hp":90,"weight":2711,"accel":15.5,"size":10,"origin":0},{"index":244,"displ":318,"hp":150,"weight":4190,"accel":13,"size":20,"origin":2},{"index":245,"displ":225,"hp":110,"weight":3360,"accel":16.6,"size":15,"origin":2},{"index":246,"displ":307,"hp":130,"weight":3504,"accel":12,"size":20,"origin":2},{"index":247,"displ":91,"hp":53,"weight":1795,"accel":17.4,"size":10,"origin":0},{"index":248,"displ":85,"hp":65,"weight":2020,"accel":19.2,"size":10,"origin":0},{"index":249,"displ":351,"hp":158,"weight":4363,"accel":13,"size":20,"origin":2},{"index":250,"displ":198,"hp":95,"weight":3102,"accel":16.5,"size":15,"origin":2},{"index":251,"displ":83,"hp":61,"weight":2003,"accel":19,"size":10,"origin":0},{"index":252,"displ":440,"hp":215,"weight":4735,"accel":11,"size":20,"origin":2},{"index":253,"displ":70,"hp":100,"weight":2420,"accel":12.5,"size":7.5,"origin":0},{"index":254,"displ":305,"hp":145,"weight":3425,"accel":13.2,"size":20,"origin":2},{"index":255,"displ":91,"hp":68,"weight":2025,"accel":18.2,"size":10,"origin":0},{"index":256,"displ":318,"hp":150,"weight":3436,"accel":11,"size":20,"origin":2},{"index":257,"displ":120,"hp":88,"weight":3270,"accel":21.9,"size":10,"origin":1},{"index":258,"displ":97,"hp":67,"weight":2065,"accel":17.8,"size":10,"origin":0},{"index":259,"displ":225,"hp":105,"weight":3121,"accel":16.5,"size":15,"origin":2},{"index":260,"displ":360,"hp":175,"weight":3821,"accel":11,"size":20,"origin":2},{"index":261,"displ":350,"hp":160,"weight":4456,"accel":13.5,"size":20,"origin":2},{"index":262,"displ":105,"hp":74,"weight":1980,"accel":15.3,"size":10,"origin":1},{"index":263,"displ":318,"hp":135,"weight":3830,"accel":15.2,"size":20,"origin":2},{"index":264,"displ":232,"hp":100,"weight":2634,"accel":13,"size":15,"origin":2},{"index":265,"displ":146,"hp":67,"weight":3250,"accel":21.8,"size":10,"origin":1},{"index":266,"displ":429,"hp":198,"weight":4341,"accel":10,"size":20,"origin":2},{"index":267,"displ":350,"hp":180,"weight":3664,"accel":11,"size":20,"origin":2},{"index":268,"displ":360,"hp":215,"weight":4615,"accel":14,"size":20,"origin":2},{"index":269,"displ":250,"hp":100,"weight":3278,"accel":18,"size":15,"origin":2},{"index":270,"displ":455,"hp":225,"weight":4425,"accel":10,"size":20,"origin":2},{"index":271,"displ":350,"hp":155,"weight":4360,"accel":14.9,"size":20,"origin":2},{"index":272,"displ":360,"hp":170,"weight":4654,"accel":13,"size":20,"origin":2},{"index":273,"displ":116,"hp":81,"weight":2220,"accel":16.9,"size":10,"origin":1},{"index":274,"displ":200,"hp":85,"weight":2587,"accel":16,"size":15,"origin":2},{"index":275,"displ":134,"hp":95,"weight":2515,"accel":14.8,"size":10,"origin":0},{"index":276,"displ":122,"hp":80,"weight":2451,"accel":16.5,"size":10,"origin":2},{"index":277,"displ":97,"hp":92,"weight":2288,"accel":17,"size":10,"origin":0},{"index":278,"displ":90,"hp":70,"weight":1937,"accel":14,"size":10,"origin":1},{"index":279,"displ":351,"hp":149,"weight":4335,"accel":14.5,"size":20,"origin":2},{"index":280,"displ":151,"hp":84,"weight":2635,"accel":16.4,"size":10,"origin":2},{"index":281,"displ":70,"hp":97,"weight":2330,"accel":13.5,"size":7.5,"origin":0},{"index":282,"displ":78,"hp":52,"weight":1985,"accel":19.4,"size":10,"origin":0},{"index":283,"displ":140,"hp":72,"weight":2408,"accel":19,"size":10,"origin":2},{"index":284,"displ":122,"hp":85,"weight":2310,"accel":18.5,"size":10,"origin":2},{"index":285,"displ":76,"hp":52,"weight":1649,"accel":16.5,"size":10,"origin":0},{"index":286,"displ":115,"hp":95,"weight":2694,"accel":15,"size":10,"origin":1},{"index":287,"displ":97,"hp":71,"weight":1825,"accel":12.2,"size":10,"origin":1},{"index":288,"displ":318,"hp":140,"weight":4080,"accel":13.7,"size":20,"origin":2},{"index":289,"displ":250,"hp":100,"weight":3282,"accel":15,"size":15,"origin":2},{"index":290,"displ":134,"hp":96,"weight":2702,"accel":13.5,"size":10,"origin":0},{"index":291,"displ":304,"hp":150,"weight":4257,"accel":15.5,"size":20,"origin":2},{"index":292,"displ":90,"hp":75,"weight":2125,"accel":14.5,"size":10,"origin":2},{"index":293,"displ":155,"hp":107,"weight":2472,"accel":14,"size":15,"origin":2},{"index":294,"displ":231,"hp":110,"weight":3415,"accel":15.8,"size":15,"origin":2},{"index":295,"displ":107,"hp":75,"weight":2205,"accel":14.5,"size":10,"origin":0},{"index":296,"displ":171,"hp":97,"weight":2984,"accel":14.5,"size":15,"origin":2},{"index":297,"displ":163,"hp":133,"weight":3410,"accel":15.8,"size":15,"origin":1},{"index":298,"displ":85,"hp":70,"weight":2070,"accel":18.6,"size":10,"origin":0},{"index":299,"displ":97,"hp":67,"weight":1985,"accel":16.4,"size":10,"origin":0},{"index":300,"displ":121,"hp":112,"weight":2933,"accel":14.5,"size":10,"origin":1},{"index":301,"displ":305,"hp":145,"weight":3880,"accel":12.5,"size":20,"origin":2},{"index":302,"displ":173,"hp":115,"weight":2595,"accel":11.3,"size":15,"origin":2},{"index":303,"displ":121,"hp":98,"weight":2945,"accel":14.5,"size":10,"origin":1},{"index":304,"displ":105,"hp":70,"weight":2200,"accel":13.2,"size":10,"origin":2},{"index":305,"displ":97,"hp":78,"weight":2190,"accel":14.1,"size":10,"origin":1},{"index":306,"displ":400,"hp":230,"weight":4278,"accel":9.5,"size":20,"origin":2},{"index":307,"displ":105,"hp":63,"weight":2125,"accel":14.7,"size":10,"origin":2},{"index":308,"displ":98,"hp":76,"weight":2144,"accel":14.7,"size":10,"origin":1},{"index":309,"displ":156,"hp":105,"weight":2800,"accel":14.4,"size":10,"origin":2},{"index":310,"displ":198,"hp":95,"weight":2833,"accel":15.5,"size":15,"origin":2},{"index":311,"displ":89,"hp":62,"weight":1845,"accel":15.3,"size":10,"origin":1},{"index":312,"displ":231,"hp":165,"weight":3445,"accel":13.4,"size":15,"origin":2},{"index":313,"displ":350,"hp":165,"weight":3693,"accel":11.5,"size":20,"origin":2},{"index":314,"displ":340,"hp":160,"weight":3609,"accel":8,"size":20,"origin":2},{"index":315,"displ":400,"hp":190,"weight":4325,"accel":12.2,"size":20,"origin":2},{"index":316,"displ":258,"hp":95,"weight":3193,"accel":17.8,"size":15,"origin":2},{"index":317,"displ":350,"hp":180,"weight":4499,"accel":12.5,"size":20,"origin":2},{"index":318,"displ":97,"hp":78,"weight":1940,"accel":14.5,"size":10,"origin":1},{"index":319,"displ":304,"hp":120,"weight":3962,"accel":13.9,"size":20,"origin":2},{"index":320,"displ":98,"hp":80,"weight":1915,"accel":14.4,"size":10,"origin":2},{"index":321,"displ":97,"hp":75,"weight":2265,"accel":18.2,"size":10,"origin":0},{"index":322,"displ":98,"hp":68,"weight":2155,"accel":16.5,"size":10,"origin":2},{"index":323,"displ":97,"hp":67,"weight":2145,"accel":18,"size":10,"origin":0},{"index":324,"displ":225,"hp":95,"weight":3785,"accel":19,"size":15,"origin":2},{"index":325,"displ":302,"hp":140,"weight":4294,"accel":16,"size":20,"origin":2},{"index":326,"displ":262,"hp":110,"weight":3221,"accel":13.5,"size":20,"origin":2},{"index":327,"displ":107,"hp":72,"weight":2290,"accel":17,"size":10,"origin":0},{"index":328,"displ":304,"hp":150,"weight":3433,"accel":12,"size":20,"origin":2},{"index":329,"displ":134,"hp":95,"weight":2560,"accel":14.2,"size":10,"origin":0},{"index":330,"displ":97,"hp":54,"weight":2254,"accel":23.5,"size":10,"origin":1},{"index":331,"displ":351,"hp":153,"weight":4129,"accel":13,"size":20,"origin":2},{"index":332,"displ":305,"hp":130,"weight":3840,"accel":15.4,"size":20,"origin":2},{"index":333,"displ":400,"hp":170,"weight":4668,"accel":11.5,"size":20,"origin":2},{"index":334,"displ":122,"hp":86,"weight":2220,"accel":14,"size":10,"origin":2},{"index":335,"displ":119,"hp":97,"weight":2545,"accel":17,"size":10,"origin":0},{"index":336,"displ":151,"hp":90,"weight":2735,"accel":18,"size":10,"origin":2},{"index":337,"displ":350,"hp":145,"weight":4082,"accel":13,"size":20,"origin":2},{"index":338,"displ":140,"hp":86,"weight":2790,"accel":15.6,"size":10,"origin":2},{"index":339,"displ":120,"hp":79,"weight":2625,"accel":18.6,"size":10,"origin":2},{"index":340,"displ":350,"hp":165,"weight":4209,"accel":12,"size":20,"origin":2},{"index":341,"displ":98,"hp":83,"weight":2075,"accel":15.9,"size":10,"origin":2},{"index":342,"displ":86,"hp":64,"weight":1875,"accel":16.4,"size":10,"origin":2},{"index":343,"displ":140,"hp":92,"weight":2865,"accel":16.4,"size":10,"origin":2},{"index":344,"displ":140,"hp":72,"weight":2565,"accel":13.6,"size":10,"origin":2},{"index":345,"displ":318,"hp":140,"weight":3735,"accel":13.2,"size":20,"origin":2},{"index":346,"displ":318,"hp":150,"weight":4135,"accel":13.5,"size":20,"origin":2},{"index":347,"displ":122,"hp":96,"weight":2300,"accel":15.5,"size":10,"origin":2},{"index":348,"displ":304,"hp":150,"weight":3672,"accel":11.5,"size":20,"origin":2},{"index":349,"displ":121,"hp":80,"weight":2670,"accel":15,"size":10,"origin":2},{"index":350,"displ":302,"hp":130,"weight":3870,"accel":15,"size":20,"origin":2},{"index":351,"displ":225,"hp":100,"weight":3630,"accel":17.7,"size":15,"origin":2},{"index":352,"displ":163,"hp":125,"weight":3140,"accel":13.6,"size":15,"origin":1},{"index":353,"displ":116,"hp":90,"weight":2123,"accel":14,"size":10,"origin":1},{"index":354,"displ":108,"hp":94,"weight":2379,"accel":16.5,"size":10,"origin":0},{"index":355,"displ":88,"hp":76,"weight":2065,"accel":14.5,"size":10,"origin":1},{"index":356,"displ":70,"hp":90,"weight":2124,"accel":13.5,"size":7.5,"origin":0},{"index":357,"displ":318,"hp":150,"weight":4077,"accel":14,"size":20,"origin":2},{"index":358,"displ":146,"hp":97,"weight":2815,"accel":14.5,"size":15,"origin":0},{"index":359,"displ":151,"hp":85,"weight":2855,"accel":17.6,"size":10,"origin":2},{"index":360,"displ":200,"hp":81,"weight":3012,"accel":17.6,"size":15,"origin":2},{"index":361,"displ":97,"hp":78,"weight":2300,"accel":14.5,"size":10,"origin":1},{"index":362,"displ":97,"hp":46,"weight":1950,"accel":21,"size":10,"origin":1},{"index":363,"displ":135,"hp":84,"weight":2385,"accel":12.9,"size":10,"origin":2},{"index":364,"displ":90,"hp":70,"weight":1937,"accel":14.2,"size":10,"origin":1},{"index":365,"displ":351,"hp":153,"weight":4154,"accel":13.5,"size":20,"origin":2},{"index":366,"displ":168,"hp":116,"weight":2900,"accel":12.6,"size":15,"origin":0},{"index":367,"displ":232,"hp":100,"weight":2901,"accel":16,"size":15,"origin":2},{"index":368,"displ":400,"hp":167,"weight":4906,"accel":12.5,"size":20,"origin":2},{"index":369,"displ":97,"hp":88,"weight":2279,"accel":19,"size":10,"origin":0},{"index":370,"displ":97,"hp":88,"weight":2130,"accel":14.5,"size":10,"origin":0},{"index":371,"displ":140,"hp":88,"weight":2890,"accel":17.3,"size":10,"origin":2},{"index":372,"displ":307,"hp":200,"weight":4376,"accel":15,"size":20,"origin":2},{"index":373,"displ":267,"hp":125,"weight":3605,"accel":15,"size":20,"origin":2},{"index":374,"displ":156,"hp":92,"weight":2585,"accel":14.5,"size":10,"origin":2},{"index":375,"displ":258,"hp":110,"weight":3632,"accel":18,"size":15,"origin":2},{"index":376,"displ":91,"hp":69,"weight":2130,"accel":14.7,"size":10,"origin":1},{"index":377,"displ":79,"hp":67,"weight":1963,"accel":15.5,"size":10,"origin":1},{"index":378,"displ":232,"hp":90,"weight":3265,"accel":18.2,"size":15,"origin":2},{"index":379,"displ":318,"hp":150,"weight":3755,"accel":14,"size":20,"origin":2},{"index":380,"displ":199,"hp":90,"weight":2648,"accel":15,"size":15,"origin":2},{"index":381,"displ":90,"hp":71,"weight":2223,"accel":16.5,"size":10,"origin":1},{"index":382,"displ":231,"hp":105,"weight":3425,"accel":16.9,"size":15,"origin":2},{"index":383,"displ":89,"hp":62,"weight":2050,"accel":17.3,"size":10,"origin":0},{"index":384,"displ":120,"hp":88,"weight":2160,"accel":14.5,"size":10,"origin":0},{"index":385,"displ":156,"hp":122,"weight":2807,"accel":13.5,"size":15,"origin":0},{"index":386,"displ":85,"hp":65,"weight":1975,"accel":19.4,"size":10,"origin":0},{"index":387,"displ":250,"hp":88,"weight":3021,"accel":16.5,"size":15,"origin":2},{"index":388,"displ":151,"hp":90,"weight":2950,"accel":17.3,"size":10,"origin":2},{"index":389,"displ":98,"hp":68,"weight":2135,"accel":16.6,"size":10,"origin":0},{"index":390,"displ":250,"hp":110,"weight":3520,"accel":16.4,"size":15,"origin":2},{"index":391,"displ":140,"hp":88,"weight":2720,"accel":15.4,"size":10,"origin":2}]},"total_rows":392,"truncation_type":null},"text/plain":"     displ   hp  weight  accel  size  origin\n0    250.0   88    3139   14.5  15.0       2\n1    304.0  193    4732   18.5  20.0       2\n2     91.0   60    1800   16.4  10.0       0\n3    250.0   98    3525   19.0  15.0       2\n4     97.0   78    2188   15.8  10.0       1\n..     ...  ...     ...    ...   ...     ...\n387  250.0   88    3021   16.5  15.0       2\n388  151.0   90    2950   17.3  10.0       2\n389   98.0   68    2135   16.6  10.0       0\n390  250.0  110    3520   16.4  15.0       2\n391  140.0   88    2720   15.4  10.0       2\n\n[392 rows x 6 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>displ</th>\n      <th>hp</th>\n      <th>weight</th>\n      <th>accel</th>\n      <th>size</th>\n      <th>origin</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>250.0</td>\n      <td>88</td>\n      <td>3139</td>\n      <td>14.5</td>\n      <td>15.0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>304.0</td>\n      <td>193</td>\n      <td>4732</td>\n      <td>18.5</td>\n      <td>20.0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>91.0</td>\n      <td>60</td>\n      <td>1800</td>\n      <td>16.4</td>\n      <td>10.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>250.0</td>\n      <td>98</td>\n      <td>3525</td>\n      <td>19.0</td>\n      <td>15.0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>97.0</td>\n      <td>78</td>\n      <td>2188</td>\n      <td>15.8</td>\n      <td>10.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>387</th>\n      <td>250.0</td>\n      <td>88</td>\n      <td>3021</td>\n      <td>16.5</td>\n      <td>15.0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>388</th>\n      <td>151.0</td>\n      <td>90</td>\n      <td>2950</td>\n      <td>17.3</td>\n      <td>10.0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>389</th>\n      <td>98.0</td>\n      <td>68</td>\n      <td>2135</td>\n      <td>16.6</td>\n      <td>10.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>390</th>\n      <td>250.0</td>\n      <td>110</td>\n      <td>3520</td>\n      <td>16.4</td>\n      <td>15.0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>391</th>\n      <td>140.0</td>\n      <td>88</td>\n      <td>2720</td>\n      <td>15.4</td>\n      <td>10.0</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n<p>392 rows × 6 columns</p>\n</div>"},"metadata":{},"execution_count":4}]},{"source":"# Import DecisionTreeRegressor\nfrom sklearn.tree import DecisionTreeRegressor\n# Import train_test_split \nfrom sklearn.model_selection import train_test_split\n# Import mean_squared_error as MSE\nfrom sklearn.metrics import mean_squared_error as MSE\n# Split data into 80% train and 20% test\nX_train, X_test, y_train, y_test= train_test_split(X, y,                                                    test_size=0.2,random_state=3)\n","metadata":{"executionCancelledAt":null,"executionTime":105,"lastExecutedAt":1695722167655,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import DecisionTreeRegressor\nfrom sklearn.tree import DecisionTreeRegressor\n# Import train_test_split \nfrom sklearn.model_selection import train_test_split\n# Import mean_squared_error as MSE\nfrom sklearn.metrics import mean_squared_error as MSE\n# Split data into 80% train and 20% test\nX_train, X_test, y_train, y_test= train_test_split(X, y,                                                    test_size=0.2,random_state=3)\n"},"cell_type":"code","id":"74bf7d67-164b-4ae7-b8c9-3358dcefc7c2","execution_count":6,"outputs":[]},{"source":"# Import DecisionTreeRegressor from sklearn.tree\nfrom sklearn.tree import DecisionTreeRegressor\n\n# Instantiate dt\ndt = DecisionTreeRegressor(max_depth=8,\n             min_samples_leaf=0.13,\n            random_state=3)\n\n# Fit dt to the training set\ndt.fit(X_train, y_train)","metadata":{"executionCancelledAt":null,"executionTime":49,"lastExecutedAt":1695717386250,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import DecisionTreeRegressor from sklearn.tree\nfrom sklearn.tree import DecisionTreeRegressor\n\n# Instantiate dt\ndt = DecisionTreeRegressor(max_depth=8,\n             min_samples_leaf=0.13,\n            random_state=3)\n\n# Fit dt to the training set\ndt.fit(X_train, y_train)"},"cell_type":"code","id":"c9e2715c-6dcc-4674-b570-def14db40d16","execution_count":39,"outputs":[{"output_type":"execute_result","data":{"text/plain":"DecisionTreeRegressor(max_depth=8, min_samples_leaf=0.13, random_state=3)","text/html":"<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeRegressor(max_depth=8, min_samples_leaf=0.13, random_state=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeRegressor</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeRegressor(max_depth=8, min_samples_leaf=0.13, random_state=3)</pre></div></div></div></div></div>"},"metadata":{},"execution_count":39}]},{"source":"# Import mean_squared_error from sklearn.metrics as MSE\nfrom sklearn.metrics import mean_squared_error as MSE\n\n# Compute y_pred\ny_pred = dt.predict(X_test)\n\n# Compute mse_dt\nmse_dt = MSE(y_pred, y_test)\n\n# Compute rmse_dt\nrmse_dt = np.sqrt(mse_dt)\n\n# Print rmse_dt\nprint(\"Test set RMSE of dt: {:.2f}\".format(rmse_dt))","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":37,"type":"stream"}}},"cell_type":"code","id":"144314dd-4142-4795-a5ec-9016224eff6e","execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":"Test set RMSE of dt: 4.37\n"}]},{"source":"## Bias-Variance Trade-off\n\nThe bias-variance trade-off is a fundamental concept in machine learning that deals with the relationship between the bias and variance of a model.\n\n**Bias** refers to the error introduced by approximating a real-world problem with a simplified model. A model with high bias tends to oversimplify the problem and make strong assumptions, leading to underfitting. Underfitting occurs when a model is unable to capture the underlying patterns in the data.\n\n**Variance** refers to the variability of a model's predictions for different training sets. A model with high variance is sensitive to the specific training data and tends to overfit. Overfitting occurs when a model becomes too complex and starts to memorize the training data instead of learning the general patterns.\n\nThe goal in machine learning is to find the right balance between bias and variance. A model with high bias may have low accuracy on both the training and test data, while a model with high variance may have high accuracy on the training data but poor generalization to new, unseen data.\n\nTo summarize:\n\n- High bias models are simple and make strong assumptions, leading to underfitting.\n- High variance models are complex and sensitive to training data, leading to overfitting.\n- The bias-variance trade-off aims to find the optimal level of complexity that minimizes both bias and variance, resulting in a model that generalizes well to new data.\n\nPerform K-Fold cross-validation to determine effectiveness of model.If CV error > training error, then model has overfitted and suffers from high variance.","metadata":{},"cell_type":"markdown","id":"bac514f3-e7c6-405b-a5cc-47c18f57e654"},{"source":"# Import train_test_split from sklearn.model_selection\nfrom sklearn.model_selection import train_test_split, cross_val_score\n\n# Set SEED for reproducibility\nSEED = 1\n\n# Split the data into 70% train and 30% test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=SEED)\n\n# Instantiate a DecisionTreeRegressor dt\ndt = DecisionTreeRegressor(max_depth=4, min_samples_leaf=0.26, random_state=SEED)\n\n# Compute the array containing the 10-folds CV MSEs\nMSE_CV_scores = - cross_val_score(dt, X_train, y_train, cv=10, \n                       scoring='neg_mean_squared_error',\n                       n_jobs=-1)\n\n# Compute the 10-folds CV RMSE\nRMSE_CV = (MSE_CV_scores.mean())**(0.5)\n\n# Print RMSE_CV\nprint('CV RMSE: {:.2f}'.format(RMSE_CV))","metadata":{"executionCancelledAt":null,"executionTime":882,"lastExecutedAt":1695722365482,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import train_test_split from sklearn.model_selection\nfrom sklearn.model_selection import train_test_split, cross_val_score\n\n# Set SEED for reproducibility\nSEED = 1\n\n# Split the data into 70% train and 30% test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=SEED)\n\n# Instantiate a DecisionTreeRegressor dt\ndt = DecisionTreeRegressor(max_depth=4, min_samples_leaf=0.26, random_state=SEED)\n\n# Compute the array containing the 10-folds CV MSEs\nMSE_CV_scores = - cross_val_score(dt, X_train, y_train, cv=10, \n                       scoring='neg_mean_squared_error',\n                       n_jobs=-1)\n\n# Compute the 10-folds CV RMSE\nRMSE_CV = (MSE_CV_scores.mean())**(0.5)\n\n# Print RMSE_CV\nprint('CV RMSE: {:.2f}'.format(RMSE_CV))","outputsMetadata":{"0":{"height":37,"type":"stream"}}},"cell_type":"code","id":"9f690d3a-0342-4a56-af79-f3c5ac67c7d1","execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":"CV RMSE: 5.14\n"}]},{"source":"# Import mean_squared_error from sklearn.metrics as MSE\nfrom sklearn.metrics import mean_squared_error as MSE\n\n# Fit dt to the training set\ndt.fit(X_train, y_train)\n\n# Predict the labels of the training set\ny_pred_train = dt.predict(X_train)\n\n# Evaluate the training set RMSE of dt\nRMSE_train = (MSE(y_train, y_pred_train))**(0.5)\n\n# Print RMSE_train\nprint('Train RMSE: {:.2f}'.format(RMSE_train))","metadata":{"executionCancelledAt":null,"executionTime":52,"lastExecutedAt":1695722513225,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import mean_squared_error from sklearn.metrics as MSE\nfrom sklearn.metrics import mean_squared_error as MSE\n\n# Fit dt to the training set\ndt.fit(X_train, y_train)\n\n# Predict the labels of the training set\ny_pred_train = dt.predict(X_train)\n\n# Evaluate the training set RMSE of dt\nRMSE_train = (MSE(y_train, y_pred_train))**(0.5)\n\n# Print RMSE_train\nprint('Train RMSE: {:.2f}'.format(RMSE_train))","outputsMetadata":{"0":{"height":37,"type":"stream"}}},"cell_type":"code","id":"399506a6-6ac6-4968-8113-f19ba4914b27","execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":"Train RMSE: 5.15\n"}]},{"source":"The training error is roughly equal to the 10-folds CV error obtained in the previous exercise, however ```baseline_RMSE``` = 5.1, therefore the model suffers from high bias because both training and CV error are roughly the same and greater than the baseline error.","metadata":{},"cell_type":"markdown","id":"06c3d903-e010-47b2-8730-040d8d3e47a9"},{"source":"## Ensemble Classification\n\nEnsemble classification is a machine learning technique that combines multiple individual classifiers to make predictions. The idea behind ensemble classification is that by combining the predictions of multiple classifiers, the overall performance can be improved compared to using a single classifier.\n\nThere are different methods for ensemble classification, such as bagging, boosting, and stacking. These methods vary in how the individual classifiers are trained and combined.\n\n## Hard Voting\n\nHard voting is a simple and commonly used method for combining the predictions of multiple classifiers in ensemble classification. In hard voting, each classifier in the ensemble makes a prediction, and the final prediction is determined by majority voting. The class label that receives the most votes from the classifiers is selected as the final prediction.\n\nHard voting can be effective when the individual classifiers in the ensemble are diverse and make independent errors. By combining their predictions, the ensemble can achieve better overall accuracy and robustness.\n","metadata":{},"cell_type":"markdown","id":"eee25070-0cca-45eb-a9da-83ea8a4c5d02"},{"source":"# Import functions to compute accuracy and split data\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n# Import models, including VotingClassifier meta-model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier as KNN\nfrom sklearn.ensemble import VotingClassifier\n\n# Set seed for reproducibility\nSEED=1\n\n# Instantiate lr\nlr = LogisticRegression(random_state=SEED)\n\n# Instantiate knn\nknn = KNN(n_neighbors=27)\n\n# Instantiate dt\ndt = DecisionTreeClassifier(min_samples_leaf=0.13, random_state=SEED)\n\n# Define the list classifiers\nclassifiers = [('Logistic Regression', lr), ('K Nearest Neighbours', knn), ('Classification Tree', dt)]\n","metadata":{"executionCancelledAt":null,"executionTime":21,"lastExecutedAt":1695723429585,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import functions to compute accuracy and split data\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n# Import models, including VotingClassifier meta-model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier as KNN\nfrom sklearn.ensemble import VotingClassifier\n\n# Set seed for reproducibility\nSEED=1\n\n# Instantiate lr\nlr = LogisticRegression(random_state=SEED)\n\n# Instantiate knn\nknn = KNN(n_neighbors=27)\n\n# Instantiate dt\ndt = DecisionTreeClassifier(min_samples_leaf=0.13, random_state=SEED)\n\n# Define the list classifiers\nclassifiers = [('Logistic Regression', lr), ('K Nearest Neighbours', knn), ('Classification Tree', dt)]\n"},"cell_type":"code","id":"cd2b0512-395f-41ec-ab05-341b368d48a7","execution_count":12,"outputs":[]},{"source":"```\n# Iterate over the pre-defined list of classifiers\nfor clf_name, clf in classifiers:    \n \n    # Fit clf to the training set\n    clf.fit(X_train, y_train)    \n   \n    # Predict y_pred\n    y_pred = clf.predict(X_test)\n    \n    # Calculate accuracy\n    accuracy = accuracy_score(y_pred, y_test) \n   \n    # Evaluate clf's accuracy on the test set\n    print('{:s} : {:.3f}'.format(clf_name, accuracy))\n    \nLogistic Regression : 0.741\nK Nearest Neighbours : 0.701\nClassification Tree : 0.707\n\n# Import VotingClassifier from sklearn.ensemble\nfrom sklearn.ensemble import VotingClassifier\n\n# Instantiate a VotingClassifier vc\nvc = VotingClassifier(estimators=classifiers)     \n\n# Fit vc to the training set\nvc.fit(X_train, y_train)   \n\n# Evaluate the test set predictions\ny_pred = vc.predict(X_test)\n\n# Calculate accuracy score\naccuracy = accuracy_score(y_pred, y_test)\nprint('Voting Classifier: {:.3f}'.format(accuracy))\n\nVoting Classifier: 0.764\n```","metadata":{},"cell_type":"markdown","id":"8b6262a6-5540-439e-ab52-6467ca0a2b10"},{"source":"**Bootstrapping** and **bagging** are techniques in machine learning that involve resampling data to improve the performance and stability of models, especially in ensemble methods like Random Forest. Here's a brief explanation of each:\n\n### Bootstrapping:\n\nBootstrapping is a statistical resampling technique where multiple datasets of the same size are created by randomly sampling data points with replacement from the original dataset.\nThis process generates multiple subsets (bootstrapped samples) that may contain duplicate data points and omit others.\nBootstrapping is commonly used for estimating statistical properties of a dataset and can be applied to train multiple models, each on a different bootstrapped sample, to assess model stability and variability.\n\n\n### Bagging (Bootstrap Aggregating):\n\nBagging is an ensemble machine learning technique that uses bootstrapping to improve model performance and reduce overfitting.\nIn bagging, multiple base models (e.g., decision trees) are trained independently on different bootstrapped samples from the training data.\nPredictions from these base models are then combined, often by averaging or voting, to make a final prediction.\nBagging helps reduce the variance of the model, making it more robust and less prone to overfitting, which can lead to improved generalization on unseen data.","metadata":{},"cell_type":"markdown","id":"81ac9626-6da7-4e4e-82b9-d6d88d7e37de"},{"source":"```\n# Import DecisionTreeClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Import BaggingClassifier\nfrom sklearn.ensemble import BaggingClassifier\n\n# Instantiate dt\ndt = DecisionTreeClassifier(random_state=1)\n\n# Instantiate bc\nbc = BaggingClassifier(base_estimator=dt, n_estimators=50, random_state=1)\n\n# Fit bc to the training set\nbc.fit(X_train, y_train)\n\n# Predict test set labels\ny_pred = bc.predict(X_test)\n\n# Evaluate acc_test\nacc_test = accuracy_score(y_pred, y_test)\nprint('Test set accuracy of bc: {:.2f}'.format(acc_test)) \n\nTest set accuracy of bc: 0.67\n```","metadata":{},"cell_type":"markdown","id":"9611556a-3617-46d1-b7dd-2446838497bf"},{"source":"**Out-of-Bag (OOB) Evaluation** is used to estimate the performance of an ensemble model, such as Random Forest, without the need for a separate validation dataset. \n\n- When building an ensemble model through bagging (Bootstrap Aggregating), multiple base models (e.g., decision trees) are trained on different bootstrapped subsets of the training data.\n- Not all data points from the original dataset are included in each bootstrap sample. Some data points are left out or remain \"out of the bag.\"\n- OOB evaluation takes advantage of these out-of-bag data points. For each base model, the data points that were not included in its respective bootstrap sample are used to evaluate the model's performance. Essentially, these out-of-bag data points serve as a validation set for the model.\n- OOB evaluations from all base models are then aggregated, typically through averaging or voting, to provide an overall estimate of the ensemble model's performance.\n- OOB evaluation is valuable because it gives an unbiased estimate of how well the ensemble model is likely to perform on unseen data, without the need for a separate validation set. This makes it a convenient and efficient method for assessing the quality of bagged models and helps in hyperparameter tuning or model selection.\n\n```\n# Import DecisionTreeClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Import BaggingClassifier\nfrom sklearn.ensemble import BaggingClassifier\n\n# Instantiate dt\ndt = DecisionTreeClassifier(min_samples_leaf=8, random_state=1)\n\n# Instantiate bc\nbc = BaggingClassifier(base_estimator=dt, \n            n_estimators=50,\n            oob_score=True,\n            random_state=1)\n            \n# Fit bc to the training set \nbc.fit(X_train, y_train)\n\n# Predict test set labels\ny_pred = bc.predict(X_test)\n\n# Evaluate test set accuracy\nacc_test = accuracy_score(y_pred, y_test)\n\n# Evaluate OOB accuracy\nacc_oob = bc.oob_score_\n\n# Print acc_test and acc_oob\nprint('Test set accuracy: {:.3f}, OOB accuracy: {:.3f}'.format(acc_test, acc_oob))\n\nTest set accuracy: 0.698, OOB accuracy: 0.702\n```","metadata":{},"cell_type":"markdown","id":"8206f9a5-67cd-4da9-885f-9ae0a5b10bc7"},{"source":"\n**Random Forests** is based on the concept of bagging (Bootstrap Aggregating) and decision trees.\n\n- Ensemble Technique: Random Forests is an ensemble learning method that combines multiple decision trees to make predictions. Each decision tree is trained on a different subset of the data using a technique called bootstrapping.\n\n- Bootstrapping: Bootstrapping involves creating multiple random subsets of the original dataset, with replacement. Each decision tree is trained on one of these subsets. Some data points are included multiple times in a subset, while others may be left out.\n\n- Random Feature Selection: In addition to bootstrapping, Random Forests introduce randomness by selecting a random subset of features at each node of the decision tree. This helps decorrelate the trees and reduces overfitting.\n\n- Voting or Averaging: Once all the decision trees are trained, they can be used to make predictions on new, unseen data. For classification tasks, the mode (most frequent class) of the individual tree predictions is taken as the final prediction. For regression tasks, the predictions from all trees are averaged.\n\n- Reduced Variance: The key advantage of Random Forests is that they tend to have lower variance compared to a single decision tree. This makes them less prone to overfitting and more robust when dealing with noisy or complex datasets.\n\n- Highly Effective: Random Forests are known for their high predictive accuracy and are widely used in various machine learning applications, including classification, regression, and feature selection.","metadata":{},"cell_type":"markdown","id":"42e56c24-18be-47e9-9cb0-9254d3ddf491"},{"source":"```\n# Import RandomForestRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Instantiate rf\nrf = RandomForestRegressor(n_estimators=25,\n            random_state=2)\n            \n# Fit rf to the training set    \nrf.fit(X_train, y_train) \n\n# Import mean_squared_error as MSE\nfrom sklearn.metrics import mean_squared_error as MSE\n\n# Predict the test set labels\ny_pred = rf.predict(X_test)\n\n# Evaluate the test set RMSE\nrmse_test = MSE(y_test, y_pred)**0.5\n\n# Print rmse_test\nprint('Test set RMSE of rf: {:.2f}'.format(rmse_test))\n\n# Create a pd.Series of features importances\nimportances = pd.Series(data=rf.feature_importances_,\n                        index= X_train.columns)\n\n# Sort importances\nimportances_sorted = importances.sort_values()\n\n# Draw a horizontal barplot of importances_sorted\nimportances_sorted.plot(kind='barh', color='lightgreen')\nplt.title('Features Importances')\nplt.show()\n\n![Uploading image.png]()\n```","metadata":{},"cell_type":"markdown","id":"bbc7bd1d-b605-451b-90f7-13c251252814"},{"source":"**Boosting** is a machine learning ensemble technique that combines multiple weak learners to create a strong learner. It is a sequential process where each weak learner is trained to correct the mistakes made by the previous weak learners. The final prediction is made by combining the predictions of all the weak learners.\n\n**AdaBoost (Adaptive Boosting)** is a specific implementation of boosting. In AdaBoost, each weak learner is assigned a weight based on its performance. The weak learners with higher weights have more influence on the final prediction. AdaBoost iteratively trains weak learners on different subsets of the training data, adjusting the weights of the training instances to focus on the difficult-to-classify examples. The final prediction is made by combining the predictions of all the weak learners, weighted by their individual performance.\n\n\n```\n# Import DecisionTreeClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Import AdaBoostClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\n\n# Instantiate dt\ndt = DecisionTreeClassifier(max_depth=2, random_state=1)\n\n# Instantiate ada\nada = AdaBoostClassifier(base_estimator=dt, n_estimators=180, random_state=1)\n\n# Fit ada to the training set\nada.fit(X_train, y_train)\n\n# Compute the probabilities of obtaining the positive class\ny_pred_proba = ada.predict_proba(X_test)[:,1]\n\n# Import roc_auc_score\nfrom sklearn.metrics import roc_auc_score\n\n# Evaluate test-set roc_auc_score\nada_roc_auc = roc_auc_score(y_test, y_pred_proba)\n\n# Print roc_auc_score\nprint('ROC AUC score: {:.2f}'.format(ada_roc_auc))\n\nROC AUC score: 0.70\n```","metadata":{},"cell_type":"markdown","id":"2bf61dc6-360e-4539-8d85-6310ad548b6a"},{"source":"## Gradient Boosting\n\nGradient Boosting is a machine learning technique that combines multiple weak models (typically decision trees) to create a strong predictive model. It is a type of ensemble learning method where each weak model is trained to correct the mistakes made by the previous models.\n\nThe main idea behind gradient boosting is to iteratively add new models to the ensemble, with each new model focusing on the examples that were poorly predicted by the previous models. This is achieved by fitting the new model to the residuals (the differences between the actual and predicted values) of the previous models.\n\nThe key concept in gradient boosting is the use of a loss function and an optimization algorithm to minimize the loss. The loss function measures the difference between the predicted and actual values, and the optimization algorithm determines how the new model is fitted to the residuals.\n\nGradient boosting has become a popular technique in various machine learning tasks, such as regression, classification, and ranking. It is known for its ability to handle complex relationships and produce accurate predictions.\n\nSome popular implementations of gradient boosting include XGBoost, LightGBM, and CatBoost.","metadata":{},"cell_type":"markdown","id":"f226fe25-4e69-4b7b-91ae-c0ade4460d8a"},{"source":"```\n# Import GradientBoostingRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n# Instantiate gb\ngb = GradientBoostingRegressor(max_depth=4, \n            n_estimators=200,\n            random_state=2)\n            \n# Fit gb to the training set\ngb.fit(X_train,y_train)\n\n# Predict test set labels\ny_pred = gb.predict(X_test)\n\n# Import mean_squared_error as MSE\nfrom sklearn.metrics import mean_squared_error as MSE\n\n# Compute MSE\nmse_test = MSE(y_pred, y_test)\n\n# Compute RMSE\nrmse_test = mse_test ** 0.5\n\n# Print RMSE\nprint('Test set RMSE of gb: {:.3f}'.format(rmse_test))\n\nTest set RMSE of gb: 52.071\n```","metadata":{},"cell_type":"markdown","id":"5924d61a-1cb6-4c0f-81cc-5c0466df28c4"},{"source":"## Stochastic Gradient Boosting\n\nStochastic Gradient Boosting is an extension of the Gradient Boosting algorithm that introduces randomness into the training process. Instead of using the entire training set to train each base learner, stochastic gradient boosting randomly selects a subset of the training data for each base learner. This introduces variability and helps to reduce overfitting.\n\nThe main steps in stochastic gradient boosting are as follows:\n\n1. Initialize the model with a constant value.\n2. For each iteration:\n   - Sample a random subset of the training data.\n   - Fit a base learner to the sampled data.\n   - Compute the negative gradient of the loss function with respect to the current model predictions.\n   - Update the model by adding a scaled version of the negative gradient.\n3. Repeat steps 2 until a specified number of iterations or a stopping criterion is reached.\n\nStochastic gradient boosting can be used for both regression and classification problems. It is a powerful algorithm that often achieves better performance than traditional gradient boosting, especially when dealing with large datasets.\n","metadata":{},"cell_type":"markdown","id":"c3f538b0-f074-4f59-829c-3cd0100ec707"},{"source":"```\n# Import GradientBoostingRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n# Instantiate sgbr\nsgbr = GradientBoostingRegressor(max_depth=4, \n            subsample=0.9,\n            max_features=0.75,\n            n_estimators=200,\n            random_state=2)\n            \n# Fit sgbr to the training set\nsgbr.fit(X_train, y_train)\n\n# Predict test set labels\ny_pred = sgbr.predict(X_test)\n\n# Import mean_squared_error as MSE\nfrom sklearn.metrics import mean_squared_error as MSE\n\n# Compute test set MSE\nmse_test = MSE(y_test,y_pred)\n\n# Compute test set RMSE\nrmse_test = mse_test ** 0.5\n\n# Print rmse_test\nprint('Test set RMSE of sgbr: {:.3f}'.format(rmse_test))\n\nTest set RMSE of sgbr: 49.621\n```","metadata":{},"cell_type":"markdown","id":"e83097f2-b0c7-49f7-9898-bf5be1741308"},{"source":"## Tuning Hyperparameters for CART Models\n\nWhen working with decision tree models, it is important to tune the hyperparameters to optimize the model's performance. In this section, we will explore how to tune the hyperparameters for CART (Classification and Regression Trees) models.\n\nThere are several hyperparameters that can be tuned for CART models, including:\n\n- `max_depth`: The maximum depth of the tree. A deeper tree can capture more complex relationships in the data, but it can also lead to overfitting.\n- `min_samples_split`: The minimum number of samples required to split an internal node. Increasing this value can prevent overfitting.\n- `min_samples_leaf`: The minimum number of samples required to be at a leaf node. Increasing this value can prevent overfitting.\n- `max_features`: The number of features to consider when looking for the best split. A smaller value can reduce overfitting.\n\nTo tune these hyperparameters, we can use techniques such as grid search or random search. Grid search involves specifying a grid of hyperparameter values and evaluating the model's performance for each combination of values. Random search involves randomly sampling from a distribution of hyperparameter values and evaluating the model's performance.\n\nLet's see an example of how to tune the hyperparameters for a CART model using grid search.\n","metadata":{},"cell_type":"markdown","id":"02c2d521-39da-4032-a304-4c2c9310f03f"},{"source":"# Import necessary libraries\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import GridSearchCV\n\n# Create a decision tree regressor\ndt = DecisionTreeRegressor()\n\n# Define the hyperparameter grid\nparam_grid = {\n    'max_depth': [3, 5, 7],\n    'min_samples_split': [2, 4, 6],\n    'min_samples_leaf': [1, 2, 3],\n    'max_features': [None, 'sqrt', 'log2']\n}\n\n# Perform grid search\ngrid_search = GridSearchCV(dt, param_grid, cv=5)\ngrid_search.fit(X_train, y_train)\n\n# Get the best hyperparameters\nbest_params = grid_search.best_params_\n\n# Print the best hyperparameters\nprint('Best Hyperparameters:', best_params)","metadata":{},"cell_type":"code","id":"160ee08e-d31c-48e5-ac79-7921f2125635","execution_count":0,"outputs":[]},{"source":"## Tuning Random Forest Hyperparameters\n\nTuning the hyperparameters of a Random Forest model is an important step in optimizing its performance. By adjusting the hyperparameters, we can find the best combination of settings that result in the most accurate and robust model.\n\nThere are several hyperparameters that can be tuned in a Random Forest model, including:\n\n- `n_estimators`: The number of trees in the forest.\n- `max_depth`: The maximum depth of each tree.\n- `min_samples_split`: The minimum number of samples required to split an internal node.\n- `min_samples_leaf`: The minimum number of samples required to be at a leaf node.\n- `max_features`: The number of features to consider when looking for the best split.\n\nTo tune the hyperparameters, we can use techniques such as grid search or random search. Grid search involves specifying a grid of possible values for each hyperparameter and evaluating the model's performance for each combination of values. Random search involves randomly sampling from the hyperparameter space and evaluating the model's performance for each sampled combination of values.\n\nLet's see an example of how to tune the hyperparameters of a Random Forest model using grid search.","metadata":{},"cell_type":"markdown","id":"48805470-5b1e-46e0-8033-cc3115cf2505"},{"source":"# Import necessary libraries\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\n\n# Create a Random Forest regressor\nrf = RandomForestRegressor()\n\n# Define the hyperparameter grid\nparam_grid = {\n    'n_estimators': [100, 200, 300],\n    'max_depth': [3, 5, 7],\n    'min_samples_split': [2, 4, 6],\n    'min_samples_leaf': [1, 2, 3],\n    'max_features': ['auto', 'sqrt', 'log2']\n}\n\n# Perform grid search\ngrid_search = GridSearchCV(rf, param_grid, cv=5)\ngrid_search.fit(X_train, y_train)\n\n# Get the best hyperparameters\nbest_params = grid_search.best_params_\n\n# Print the best hyperparameters\nprint('Best Hyperparameters:', best_params)","metadata":{},"cell_type":"code","id":"ad4e6843-11cd-4d37-bea7-ad63c7a8b118","execution_count":0,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.8.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"editor":"DataCamp Workspace"},"nbformat":4,"nbformat_minor":5}