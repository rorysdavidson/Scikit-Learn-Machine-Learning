{"cells":[{"source":"## Linear Classifiers\n\nLinear classifiers are a type of supervised machine learning algorithm that are used for classification tasks. They aim to find a linear decision boundary that separates the data points into different classes.\n\nThere are several types of linear classifiers, including:\n\n- Logistic Regression\n- Support Vector Machines (SVM)\n- Perceptron\n\nThese classifiers make predictions based on a linear combination of the input features, where the coefficients of the linear combination are learned during the training process.\n\nLinear classifiers are often used when the data is linearly separable, meaning that a straight line or hyperplane can separate the data points of different classes. However, they can also be used for non-linearly separable data by applying non-linear transformations to the input features.\n\nTo train a linear classifier, you typically need a labeled dataset, where each data point is associated with a class label. The classifier learns from this labeled data to make predictions on new, unseen data points.","metadata":{},"cell_type":"markdown","id":"4d62af23-0ece-42a4-82c5-93c56fc0a279"},{"source":"## Support Vector Machines (SVM)\n\nSupport Vector Machines (SVM) is a supervised machine learning algorithm that can be used for both classification and regression tasks. It is based on the concept of finding the hyperplane that best separates the data points into different classes.\n\nIn SVM, the goal is to find the optimal hyperplane that maximizes the margin between the support vectors, which are the data points closest to the decision boundary. The decision boundary is defined by the hyperplane, and the margin is the distance between the hyperplane and the support vectors.\n\nSVM can handle both linearly separable and non-linearly separable data by using different types of kernels. Kernels are functions that transform the input data into a higher-dimensional space, where it becomes easier to find a hyperplane that separates the data.\n\nSVM has several advantages, including:\n\n- Effective in high-dimensional spaces\n- Memory efficient\n- Versatile with different kernel functions\n\nHowever, SVM can be sensitive to the choice of hyperparameters and may require careful tuning to achieve optimal performance.\n\n```\nfrom sklearn.svm import LinearSVC\n\n# Apply SVM and print scores\n    svm = LinearSVC()\n    svm.fit(X_train,y_train)\n    print(svm.score(X_train,y_train))\n    print(svm.score(X_test,y_test))\n```","metadata":{},"cell_type":"markdown","id":"d9bde777-da0d-4d24-a24d-c031785709c6"},{"source":"A subset of scikit-learn's built-in wine dataset is already loaded into X, along with binary labels in y\n```\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Define the classifiers\nclassifiers = [LogisticRegression(),LinearSVC(),SVC(),KNeighborsClassifier()]\n\n# Fit the classifiers\nfor c in classifiers:\n    c.fit(X,y)\n\n# Plot the classifiers\nplot_4_classifiers(X, y, classifiers)\nplt.show()\n```\n\n![image](image.png)\n","metadata":{},"cell_type":"markdown","id":"0f38f962-debc-47d6-a2f1-67b46a6f4d0d"},{"source":"## Minimizing Loss in Linear Classifiers\n\nIn linear classifiers, the goal is to find the optimal hyperplane that separates the data points into different classes. This hyperplane is determined by minimizing a loss function.\n\nA loss function measures the error or mismatch between the predicted class labels and the true class labels. The goal is to minimize this error by adjusting the parameters of the linear classifier.\n\nOne commonly used loss function for linear classifiers is the hinge loss function, which penalizes misclassifications and encourages a larger margin between the decision boundary and the support vectors.\n\nThe hinge loss function can be visualized in a loss diagram. The x-axis represents the margin, which is the distance between the decision boundary and the support vectors. The y-axis represents the loss, which increases as the margin decreases.\n\n![Loss Diagram](https://upload.wikimedia.org/wikipedia/commons/thumb/1/1b/Hinge_loss.svg/400px-Hinge_loss.svg.png)\n\nIn the loss diagram, the loss is zero when the margin is larger than a certain threshold. As the margin decreases, the loss increases linearly until it reaches a maximum value when the margin is zero or negative.\n\nThe goal of minimizing the loss is to find the hyperplane that maximizes the margin while keeping the loss as low as possible. This is achieved through optimization algorithms that adjust the parameters of the linear classifier based on the gradient of the loss function.\n\nBy minimizing the loss, linear classifiers can effectively separate the data points into different classes and make accurate predictions on new unseen data.","metadata":{},"cell_type":"markdown","id":"9a1b22ed-119c-4f4b-90d3-e74a82f7ecb7"},{"source":"## Regularization in Logistic Regression\n\nRegularization is a technique used in machine learning to prevent overfitting and improve the generalization of a model. In the context of logistic regression, regularization involves adding a penalty term to the loss function.\n\nThe loss function in logistic regression is typically the negative log-likelihood function, which measures the error between the predicted probabilities and the actual labels. The penalty term is added to this loss function to control the complexity of the model.\n\nThere are two commonly used types of regularization in logistic regression:\n\n1. L1 Regularization (Lasso Regression): This type of regularization adds the absolute values of the coefficients as the penalty term. It encourages sparsity in the model by shrinking some coefficients to zero.\n\n2. L2 Regularization (Ridge Regression): This type of regularization adds the squared values of the coefficients as the penalty term. It encourages small values for all coefficients, but does not force them to zero.\n\nThe regularization parameter, often denoted as lambda (Î») or c, controls the strength of the penalty term. A higher value of lambda results in stronger regularization and a simpler model, while a lower value of lambda allows the model to fit the training data more closely.\n\nRegularization helps to prevent overfitting by discouraging the model from relying too heavily on any single feature or combination of features. It can also improve the interpretability of the model by reducing the impact of irrelevant or noisy features.\n","metadata":{},"cell_type":"markdown","id":"b05490b2-9368-4ebc-9a97-6c4c93b9d4c9"},{"source":"To define logistic regression probabilities, you can use the `predict_proba` method of the logistic regression model. This method returns the predicted probabilities for each class.\n\nHere's an example:\n\n```python\n# Import the necessary libraries\nfrom sklearn.linear_model import LogisticRegression\n\n# Instantiate the logistic regression model\nlr = LogisticRegression()\n\n# Fit the model to the training data\nlr.fit(X_train, y_train)\n\n# Predict probabilities for the test data\nprobs = lr.predict_proba(X_test)\n\n# Print the predicted probabilities\nprint(probs)\n```\n\nIn this example, `X_train` and `y_train` are the training data, and `X_test` is the test data. The `predict_proba` method returns an array of shape `(n_samples, n_classes)`, where `n_samples` is the number of samples in the test data and `n_classes` is the number of classes in the target variable.\n","metadata":{},"cell_type":"markdown","id":"6ba6eca2-040d-4676-8362-c574bf8d52e4"},{"source":"```\n# Specify L1 regularization\nlr = LogisticRegression(solver='liblinear', penalty=\"l1\")\n\n# Instantiate the GridSearchCV object and run the search\nsearcher = GridSearchCV(lr, {'C':[0.001, 0.01, 0.1, 1, 10]})\nsearcher.fit(X_train, y_train)\n\n# Report the best parameters\nprint(\"Best CV params\", searcher.best_params_)\n\n# Find the number of nonzero coefficients (selected features)\nbest_lr = searcher.best_estimator_\ncoefs = best_lr.coef_\nprint(\"Total number of features:\", coefs.size)\nprint(\"Number of selected features:\", np.count_nonzero(coefs))\n\nBest CV params {'C': 1}\nTotal number of features: 2500\nNumber of selected features: 1219\n\n# Get the indices of the sorted cofficients\ninds_ascending = np.argsort(lr.coef_.flatten()) \ninds_descending = inds_ascending[::-1]\n\n# Print the most positive words\nprint(\"Most positive words: \", end=\"\")\nfor i in range(5):\n    print(vocab[inds_descending[i]], end=\", \")\nprint(\"\\n\")\n\n# Print most negative words\nprint(\"Most negative words: \", end=\"\")\nfor i in range(5):\n    print(vocab[inds_ascending[i]], end=\", \")\nprint(\"\\n\")\n\nMost positive words: favorite, superb, noir, knowing, excellent, \n\nMost negative words: worst, disappointing, waste, boring, lame, \n\n```","metadata":{},"cell_type":"markdown","id":"d95497ef-87f1-45f2-98ac-81f5a1ef20d9"},{"source":"```LogisticRegression(multi_class='ovr')``` performs 'one vs rest' regression on multi-class target variables, otherwise can perform multinomial (softmax) regression.\n\n```\n# Fit one-vs-rest logistic regression classifier\nlr_ovr = LogisticRegression(multi_class='ovr')\nlr_ovr.fit(X_train, y_train)\n\nprint(\"OVR training accuracy:\", lr_ovr.score(X_train, y_train))\nprint(\"OVR test accuracy    :\", lr_ovr.score(X_test, y_test))\n\n# Fit softmax classifier\nlr_mn = LogisticRegression(multi_class='multinomial')\nlr_mn.fit(X_train, y_train)\n\nprint(\"Softmax training accuracy:\", lr_mn.score(X_train, y_train))\nprint(\"Softmax test accuracy    :\", lr_mn.score(X_test, y_test))\n\nOVR training accuracy: 0.9955456570155902\nOVR test accuracy    : 0.9644444444444444\nSoftmax training accuracy: 1.0\nSoftmax test accuracy    : 0.9688888888888889\n\n```\n","metadata":{},"cell_type":"markdown","id":"317def9e-e11a-4f6c-b44c-239b11813d51"},{"source":"Support vectors are the data points that lie closest to the decision boundary of a support vector machine (SVM) classifier. These data points play a crucial role in defining the decision boundary and separating different classes.\n\nIn SVM, the decision boundary is determined by a subset of the training data called support vectors. These support vectors are the data points that have non-zero coefficients in the representation of the decision boundary.\n\nTo find the support vectors in scikit-learn, you can use the `support_vectors_` attribute of the trained SVM model. This attribute returns an array of shape (n_support_vectors, n_features) containing the support vectors.\n\nHere is an example of how to find the support vectors using scikit-learn:\n\n```python\n# Fit SVM classifier\nsvm = SVC(kernel='linear')\nsvm.fit(X_train, y_train)\n\n# Get support vectors\nsupport_vectors = svm.support_vectors_\n\nprint('Number of support vectors:', support_vectors.shape[0])\nprint('Support vectors:', support_vectors)\n```\n","metadata":{},"cell_type":"markdown","id":"56019dae-5369-447d-a4f2-61b9b22d95b1"},{"source":"### Jointly tuning gamma and C with GridSearchCV\n```\n# Instantiate an RBF SVM\nsvm = SVC()\n\n# Instantiate the GridSearchCV object and run the search\nparameters = {'C':[0.1, 1, 10], 'gamma':[0.00001, 0.0001, 0.001, 0.01, 0.1]}\nsearcher = GridSearchCV(svm, parameters)\nsearcher.fit(X_train,y_train)\n\n# Report the best parameters and the corresponding score\nprint(\"Best CV params\", searcher.best_params_)\nprint(\"Best CV accuracy\", searcher.best_score_)\n\n# Report the test accuracy using these best parameters\nprint(\"Test accuracy of best grid search hypers:\", searcher.score(X_test,y_test))\n\nBest CV params {'C': 1, 'gamma': 0.001}\nBest CV accuracy 0.9988826815642458\nTest accuracy of best grid search hypers: 0.9988876529477196\n```","metadata":{},"cell_type":"markdown","id":"ef367629-e0c8-4d24-9b20-c3f5bd873b4e"},{"source":"The SGD (Stochastic Gradient Descent) Classifier is a linear classifier that uses stochastic gradient descent to optimize the model parameters. It is a popular algorithm for large-scale machine learning tasks due to its efficiency and ability to handle large datasets.\n\n```\n# Import the necessary libraries\nfrom sklearn.linear_model import SGDClassifier\n\n# Instantiate the SGDClassifier\nsgd_classifier = SGDClassifier()\n\n# Print the details of the SGDClassifier\nprint(sgd_classifier)\n```\n\nHere are the key steps involved in the working of the SGD Classifier:\n\n1. **Initialization**: The model parameters (weights and biases) are initialized randomly or with some predefined values.\n\n2. **Training**: The SGD Classifier iteratively updates the model parameters using stochastic gradient descent. In each iteration, a random sample (or a mini-batch) from the training data is used to compute the gradient of the loss function with respect to the model parameters. The model parameters are then updated in the direction of the negative gradient to minimize the loss function.\n\n3. **Convergence**: The training process continues until a stopping criterion is met, such as reaching a maximum number of iterations or achieving a desired level of convergence.\n\n4. **Prediction**: Once the model is trained, it can be used to make predictions on new unseen data by computing the dot product between the input features and the learned model parameters.\n\nThe SGD Classifier is particularly useful for large-scale classification problems where the number of samples or features is very large. It can handle both sparse and dense input data and supports various loss functions and regularization techniques.\n\n```\n# We set random_state=0 for reproducibility \nlinear_classifier = SGDClassifier(random_state=0)\n\n# Instantiate the GridSearchCV object and run the search\nparameters = {'alpha':[0.00001, 0.0001, 0.001, 0.01, 0.1, 1], \n             'loss':['hinge','log_loss']}\nsearcher = GridSearchCV(linear_classifier, parameters, cv=10)\nsearcher.fit(X_train, y_train)\n\n# Report the best parameters and the corresponding score\nprint(\"Best CV params\", searcher.best_params_)\nprint(\"Best CV accuracy\", searcher.best_score_)\nprint(\"Test accuracy of best grid search hypers:\", searcher.score(X_test, y_test))\n\nBest CV params {'alpha': 0.001, 'loss': 'hinge'}\nBest CV accuracy 0.9490730158730158\nTest accuracy of best grid search hypers: 0.9611111111111111\n\n```","metadata":{},"cell_type":"markdown","id":"5eb19166-a3ad-4803-9f08-001c974473a5"}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.8.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"editor":"DataCamp Workspace"},"nbformat":4,"nbformat_minor":5}